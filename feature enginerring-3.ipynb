{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad6747ce-6622-4310-a71c-bcb24b2bed85",
   "metadata": {},
   "source": [
    "## Q1. What is Min-Max scaling, and how is it used in data preprocessing? Provide an example to illustrate its application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac5f24de-4877-47d6-beeb-0a9fb8f44b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "Min-Max scaling, also known as normalization, is a data preprocessing technique used to transform numerical features in a\n",
    "dataset into a specific range, typically between 0 and 1. This scaling method preserves the relative relationships between\n",
    "data points while ensuring that all values fall within the desired interval.\n",
    "\n",
    "The formula for Min-Max scaling is as follows:\n",
    "\n",
    "        Xnormalized= X−Xmin/Xmax−Xmin\n",
    "\n",
    "Where:\n",
    "\n",
    "    ~X is the original value of the feature.\n",
    "    ~Xnormalized is the scaled value of the feature.\n",
    "    ~Xmin is the minimum value of the feature in the dataset.\n",
    "    ~Xmax is the maximum value of the feature in the dataset.\n",
    "    \n",
    "Here's how Min-Max scaling is used in data preprocessing:\n",
    "\n",
    "1.Data Understanding: Begin by understanding the dataset and the range of values in each numerical feature. Identify the\n",
    "  minimum (Xmin) and maximum (Xmax) values for each feature.\n",
    "\n",
    "2.Scaling: Apply the Min-Max scaling formula to each data point in the feature. This transformation scales the values\n",
    "  linearly so that the minimum value becomes 0, the maximum value becomes 1, and all other values are scaled proportionally\n",
    "in between.\n",
    "\n",
    "3.Normalization Range: By default, Min-Max scaling scales values to the range [0, 1]. However, you can adjust the range to\n",
    "  fit your specific needs. For example, you might want to scale values to the range [0, 100] or [-1, 1] depending on the\n",
    "context of your analysis.\n",
    "\n",
    "4.Implementation: Implement Min-Max scaling in your data preprocessing pipeline using a programming language or library. Many\n",
    "  programming languages, such as Python, provide libraries like scikit-learn for data preprocessing, which includes Min-Max\n",
    "scaling as a built-in feature.\n",
    "\n",
    "Here's an example to illustrate the application of Min-Max scaling:\n",
    "\n",
    "Suppose you have a dataset containing the following values for a feature representing house prices:\n",
    "\n",
    "X=[250,000,500,000,750,000,1,000,000]\n",
    "\n",
    "To apply Min-Max scaling to this feature:\n",
    "\n",
    "1.Calculate Xmin and Xmax:\n",
    "\n",
    "    ~Xmin=250,000 (minimum value in the dataset)\n",
    "    ~Xmax=1,000,000 (maximum value in the dataset)\n",
    "    \n",
    "2.Apply the Min-Max scaling formula to each data point:\n",
    "\n",
    "    ~For X=250,000:\n",
    "        Xnormalized=250,000−250,000/1,000,000−250,000\n",
    "                   =0\n",
    "    ~For X=500,000:\n",
    "        Xnormalized=500,000−250,000/1,000,000−250,000\n",
    "                   =0.25\n",
    "    ~For X=750,000:\n",
    "        Xnormalized= 750,000−250,000/1,000,000−250,000\n",
    "                   =0.5\n",
    "    ~For X=1,00,000:\n",
    "        Xnormalized= 1,000,000−250,000/1,000,000−250,000\n",
    "                   =1\n",
    "\n",
    "After Min-Max scaling, the feature values are transformed into the range [0, 1]:\n",
    "\n",
    "        Xnormalized=[0,0.25,0.5,1]\n",
    "\n",
    "Min-Max scaling is beneficial in machine learning and data analysis when features have different scales, and you want to bring\n",
    "them into a standardized range to ensure that they contribute equally to the analysis without introducing bias due to\n",
    "magnitude differences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2bc901a-e60d-422e-b585-d9e463e31b23",
   "metadata": {},
   "source": [
    "## Q2. What is the Unit Vector technique in feature scaling, and how does it differ from Min-Max scaling?Provide an example to illustrate its application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77482a11-4f4a-4be6-988a-34bff0b7d9a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "The Unit Vector technique, also known as \"Normalization\" or \"Scaling to Unit Norm,\" is a feature scaling method used to\n",
    "transform numerical features in a dataset such that they have a unit norm or magnitude of 1. This scaling method normalizes\n",
    "the values of each feature to have equal importance in terms of their direction but preserves the direction or angle between\n",
    "data points. It is commonly used in machine learning algorithms that rely on distance or vector operations, such as k-nearest\n",
    "neighbors (KNN) and support vector machines (SVM).\n",
    "\n",
    "The formula for scaling a feature to a unit vector is as follows:\n",
    "\n",
    "        Xnormalized=X/∥X∥\n",
    "        \n",
    "Where:\n",
    "    \n",
    "    ~X is the original value of the feature.\n",
    "    ~Xnormalized is the scaled value of the feature with a unit norm.\n",
    "    ~∥X∥ represents the Euclidean norm or L2 norm of the feature vector, which is the square root of the sum of squares of its\n",
    "     elements.\n",
    "        \n",
    "Here's how the Unit Vector technique differs from Min-Max scaling:\n",
    "\n",
    "1.Normalization Range: Min-Max scaling scales the feature values to a specific range (e.g., [0, 1]), while the Unit Vector\n",
    "  technique scales the values to have a unit norm (magnitude of 1).\n",
    "\n",
    "2.Magnitude Preservation: Min-Max scaling preserves the relative magnitude of feature values. Features with larger values\n",
    "  will still have larger values after scaling. In contrast, the Unit Vector technique equalizes the magnitude of all features \n",
    "to 1, regardless of their original magnitudes.\n",
    "\n",
    "3.Use Case: Min-Max scaling is typically used when you want to bring feature values into a specific bounded range, whereas \n",
    "  the Unit Vector technique is used when you want to emphasize the direction or relative importance of features while\n",
    "downplaying their magnitudes.\n",
    "\n",
    "Here's an example to illustrate the application of the Unit Vector technique:\n",
    "\n",
    "Suppose you have a dataset with two numerical features representing the (x, y) coordinates of data points:\n",
    "\n",
    "    X1 =[3,4]\n",
    "    X2 =[1,2]\n",
    "\n",
    "1.To apply the Unit Vector technique to these feature vectors:\n",
    "\n",
    "    ~Calculate the Euclidean norm (∥X∥) for each feature vector:\n",
    "            For X1 :\n",
    "                ∥X1∥= 32+42 = 9+16 = 25 =5\n",
    "            For X2 :\n",
    "                ∥X2∥= 12+22 = 1+4 = 5\n",
    "\n",
    "2.Normalize each feature vector by dividing it by its Euclidean norm:\n",
    "\n",
    "    For X1 :\n",
    "        X1normalized = X1/∥X1∥ = [3,4]/5 =[0.6,0.8]\n",
    "\n",
    "    For X2 :\n",
    "        X2normalized = X2/∥X2∥ = [1,2]/5 ≈[0.447,0.894]\n",
    "\n",
    "After applying the Unit Vector technique, both feature vectors have a unit norm, and their magnitudes are equal to 1:\n",
    "\n",
    "            X1normalized =[0.6,0.8]\n",
    "            X2normalized ≈[0.447,0.894]\n",
    "\n",
    "In this example, the direction or angle between the two feature vectors is preserved, while their magnitudes have been scaled \n",
    "to 1, making them suitable for distance-based calculations or algorithms that emphasize feature direction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f40609-3d94-4cb1-881a-752fc0f60372",
   "metadata": {},
   "source": [
    "## Q3. What is PCA (Principle Component Analysis), and how is it used in dimensionality reduction? Provide an example to illustrate its application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e67a574f-1569-4d8c-b357-02dc831f5d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "Principal Component Analysis (PCA) is a dimensionality reduction technique used in data analysis and machine learning. Its\n",
    "primary goal is to reduce the dimensionality of a dataset while preserving as much of the original variance or information\n",
    "as possible. PCA accomplishes this by transforming the original features into a new set of orthogonal features called \n",
    "principal components.\n",
    "\n",
    "Here's how PCA works and its application in dimensionality reduction:\n",
    "\n",
    "1.Centering the Data:\n",
    "\n",
    "    ~PCA begins by centering the data, which means subtracting the mean of each feature from all data points. This ensures \n",
    "     that the data is centered at the origin.\n",
    "        \n",
    "2.Covariance Matrix:\n",
    "\n",
    "    ~PCA then computes the covariance matrix of the centered data. The covariance matrix quantifies how the features vary\n",
    "     together. It shows which features are positively or negatively correlated and to what extent.\n",
    "        \n",
    "3.Eigendecomposition:\n",
    "\n",
    "    ~The next step is to perform an eigendecomposition (eigenvalue decomposition) of the covariance matrix. This\n",
    "     decomposition yields eigenvalues and corresponding eigenvectors.\n",
    "    ~Eigenvalues represent the variance of the data along the principal components. Larger eigenvalues correspond to\n",
    "     directions in the feature space where the data has the most variance.\n",
    "    ~Eigenvectors represent the directions (principal components) along which the data varies the most. These vectors are\n",
    "     orthogonal to each other, ensuring that the new features are uncorrelated.\n",
    "        \n",
    "4.Selecting Principal Components:\n",
    "\n",
    "    ~After obtaining the eigenvalues and eigenvectors, you can select a subset of the principal components (eigenvectors)\n",
    "     based on your desired level of dimensionality reduction. Typically, you'll rank the eigenvalues in descending order and \n",
    "    select the top k eigenvectors, where k is the number of dimensions you want to retain.\n",
    "    \n",
    "5.Transforming the Data:\n",
    "\n",
    "    ~Finally, you transform the original data using the selected principal components. This transformation projects the data\n",
    "     into a new feature space defined by the principal components.\n",
    "    ~PCA is widely used for various purposes, including dimensionality reduction, data visualization, and noise reduction.\n",
    "     One of its primary applications is dimensionality reduction. It reduces the number of features while retaining most of \n",
    "    the relevant information in the data. Here's an example to illustrate PCA's application in dimensionality reduction:\n",
    "\n",
    "Suppose you have a dataset with three numerical features: height, weight, and age, and you want to reduce the dimensionality\n",
    "to two dimensions.\n",
    "\n",
    "Original data:\n",
    "\n",
    "Feature 1: Height (in inches)\n",
    "Feature 2: Weight (in pounds)\n",
    "Feature 3: Age (in years)\n",
    "\n",
    "1.Centering the Data:\n",
    "\n",
    "    ~Center the data by subtracting the mean of each feature from the data points.\n",
    "\n",
    "2.Covariance Matrix:\n",
    "\n",
    "    ~Compute the covariance matrix to understand how the features are correlated.\n",
    "\n",
    "3.Eigendecomposition:\n",
    "\n",
    "    ~Perform eigendecomposition to obtain eigenvalues and eigenvectors.\n",
    "    ~Let's say you obtain the following eigenvalues: λ1 =1000, λ2 =100, λ3 =1.\n",
    "    ~The corresponding eigenvectors are the principal components.\n",
    "    \n",
    "4.Selecting Principal Components:\n",
    "\n",
    "    ~You decide to retain the top two principal components (eigenvectors associated with the two largest eigenvalues).\n",
    "    \n",
    "5.Transforming the Data:\n",
    "\n",
    "    ~Multiply the original data by the selected principal components to obtain the reduced-dimensional data.\n",
    "    \n",
    "The reduced-dimensional data will have two features that capture most of the variance in the original data. This reduced\n",
    "representation can be used for further analysis, visualization, or modeling while effectively reducing dimensionality."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5356d137-7033-4db4-b423-4603184839a0",
   "metadata": {},
   "source": [
    "## Q4. What is the relationship between PCA and Feature Extraction, and how can PCA be used for Feature Extraction? Provide an example to illustrate this concept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6436a850-aa1d-430e-863f-3624a0be1e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "PCA (Principal Component Analysis) is closely related to feature extraction, and it can be used as a feature extraction \n",
    "technique. Feature extraction is the process of transforming the original features of a dataset into a new set of features \n",
    "that capture the most relevant information while reducing dimensionality. PCA achieves this by finding a new set of features\n",
    "called principal components, which are linear combinations of the original features.\n",
    "\n",
    "Here's the relationship between PCA and feature extraction, along with an example to illustrate this concept:\n",
    "\n",
    "1.Dimensionality Reduction:\n",
    "\n",
    "    ~Both PCA and feature extraction aim to reduce the dimensionality of a dataset while retaining important information.\n",
    "    ~Feature extraction techniques create new features that are a combination of the original features, effectively\n",
    "     summarizing the data in a more compact form.\n",
    "    ~PCA specifically identifies principal components, which are orthogonal linear combinations of the original features,\n",
    "     such that the first principal component captures the most variance, the second captures the second most variance, and\n",
    "    so on.\n",
    "    \n",
    "2.Orthogonality and Unrelatedness:\n",
    "\n",
    "    ~PCA ensures that the selected principal components are orthogonal to each other, meaning they are uncorrelated. This\n",
    "     property is useful in various applications, such as removing multicollinearity and simplifying feature relationships.\n",
    "    ~Feature extraction methods can also create new features that are uncorrelated, but they may not necessarily be \n",
    "     orthogonal.\n",
    "        \n",
    "3.Eigenvalues and Feature Importance:\n",
    "\n",
    "    ~PCA quantifies the importance of each principal component using eigenvalues. Larger eigenvalues correspond to more\n",
    "     important components, indicating how much variance they capture.\n",
    "    ~Some feature extraction techniques provide a measure of feature importance, but PCA's eigenvalues offer a clear ranking\n",
    "     of importance.\n",
    "        \n",
    "Here's an example to illustrate how PCA can be used for feature extraction:\n",
    "\n",
    "Suppose you have a dataset with four numerical features: feature A, feature B, feature C, and feature D. You want to reduce \n",
    "the dimensionality of the dataset by extracting two principal components.\n",
    "\n",
    "Original data:\n",
    "\n",
    "Feature A: Annual income (in dollars)\n",
    "Feature B: Age (in years)\n",
    "Feature C: Number of years of education\n",
    "Feature D: Credit score\n",
    "\n",
    "1.Standardize the Data:\n",
    "\n",
    "    ~Before applying PCA, standardize the data by subtracting the mean and dividing by the standard deviation for each \n",
    "     feature. This ensures that all features have the same scale.\n",
    "        \n",
    "2.PCA Transformation:\n",
    "\n",
    "    ~Apply PCA to the standardized data to find the two principal components.\n",
    "    ~PCA identifies two linear combinations of the original features, let's call them PC1 and PC2. These new features are \n",
    "     orthogonal and capture the most variance in the data.\n",
    "        \n",
    "3.Transform the Data:\n",
    "\n",
    "    ~Transform the original data using the identified principal components. This results in a reduced-dimensional dataset\n",
    "     with only PC1 and PC2 as features.\n",
    "        \n",
    "The new dataset now consists of two features, PC1 and PC2, which are linear combinations of the original features. PC1 and\n",
    "PC2 are uncorrelated and capture most of the variance in the original data. You can use this reduced-dimensional dataset for\n",
    "further analysis or modeling, effectively performing feature extraction with PCA."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27cbc40e-e3d4-4519-b9cf-69ef172933e6",
   "metadata": {},
   "source": [
    "## Q5. You are working on a project to build a recommendation system for a food delivery service. The dataset contains features such as price, rating, and delivery time. Explain how you would use Min-Max scaling to preprocess the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f4e4b2c-e370-40fb-9c30-68702a86f3d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "To preprocess the data for building a recommendation system for a food delivery service using Min-Max scaling, follow these\n",
    "steps:\n",
    "\n",
    "1.Data Collection and Understanding:\n",
    "\n",
    "    ~Collect and import your dataset, which contains features such as price, rating, and delivery time. Ensure you understand\n",
    "     the data's structure and the range of values for each feature.\n",
    "        \n",
    "2.Data Preprocessing:\n",
    "\n",
    "    ~Handle any missing values, outliers, or other data quality issues as needed.\n",
    "    \n",
    "3.Feature Selection (if necessary):\n",
    "\n",
    "    ~Decide which features are relevant for your recommendation system. Depending on your specific goals, you may choose to \n",
    "     use all available features or a subset of them.\n",
    "        \n",
    "4.Min-Max Scaling:\n",
    "\n",
    "    ~Apply Min-Max scaling to each feature that you decide to use. Here's how to do it for each feature separately:\n",
    "    ~Price: Suppose the original price values are in the range $10 to $50. To scale this feature using Min-Max scaling to a\n",
    "     range of [0, 1], use the formula for Min-Max scaling:\n",
    "\n",
    "            Xnormalized = X−X min/ X max−X min\n",
    "                ~X is the original price value.\n",
    "                ~X min is the minimum price value in your dataset ($10 in this case).\n",
    "                ~X max is the maximum price value in your dataset ($50 in this case).\n",
    "                \n",
    "    ~Rating: Suppose the original rating values are on a scale of 1 to 5. To scale this feature to a range of [0, 1]:\n",
    "\n",
    "            X min =1 (minimum rating value)\n",
    "            X max =5 (maximum rating value)\n",
    "            \n",
    "    ~Delivery Time: Suppose the original delivery time values are in minutes, ranging from 15 to 60 minutes. To scale this\n",
    "     feature to a range of [0, 1]:\n",
    "\n",
    "            X min =15 (minimum delivery time)\n",
    "            X max =60 (maximum delivery time)\n",
    "5.Updated Dataset:\n",
    "\n",
    "    ~After Min-Max scaling, your dataset will contain the same features (price, rating, and delivery time), but the values \n",
    "     of these features will be in the range [0, 1], which makes them comparable and suitable for modeling.\n",
    "        \n",
    "6.Building the Recommendation System:\n",
    "\n",
    "    ~With the preprocessed data, you can proceed to build your recommendation system. Depending on your project's \n",
    "     requirements, you can use various recommendation algorithms, such as collaborative filtering, content-based filtering, \n",
    "    or hybrid methods.\n",
    "    \n",
    "Min-Max scaling ensures that the features are on a consistent scale and helps prevent features with larger numerical ranges \n",
    "from dominating the recommendation process. It makes it easier for your recommendation algorithm to consider all features\n",
    "equally when making recommendations, leading to more accurate and fair recommendations for users of the food delivery service."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f30bd46-6569-4687-b205-833c08511325",
   "metadata": {},
   "source": [
    "## Q6. You are working on a project to build a model to predict stock prices. The dataset contains many features, such as company financial data and market trends. Explain how you would use PCA to reduce the dimensionality of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f7331d-647c-47a1-b458-0e8a2e1cebc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Using Principal Component Analysis (PCA) to reduce the dimensionality of a dataset for building a stock price prediction model\n",
    "can be a valuable preprocessing step. It allows you to capture the most significant patterns and relationships among the\n",
    "features while reducing the complexity of the model. Here's how you can apply PCA to achieve dimensionality reduction in your\n",
    "stock price prediction project:\n",
    "\n",
    "1.Data Collection and Understanding:\n",
    "\n",
    "    ~Collect and preprocess your dataset, which includes various features like company financial data (e.g., revenue,\n",
    "     earnings, debt) and market trends (e.g., interest rates, stock market indices).\n",
    "    ~Gain a deep understanding of the dataset and the relationships between features.\n",
    "    \n",
    "2.Feature Selection:\n",
    "\n",
    "    ~Carefully consider which features are relevant for predicting stock prices. Feature selection based on domain knowledge\n",
    "     and feature importance analysis can help you identify the most informative features.\n",
    "        \n",
    "3.Standardize the Data:\n",
    "\n",
    "    ~Standardize the data by subtracting the mean and dividing by the standard deviation for each feature. Standardization\n",
    "     ensures that all features have the same scale, which is a prerequisite for PCA.\n",
    "        \n",
    "4.PCA Transformation:\n",
    "\n",
    "    ~Apply PCA to the standardized dataset to reduce dimensionality. PCA identifies a set of orthogonal components (principal \n",
    "     components) that capture the most variance in the data.\n",
    "    ~Specify the number of principal components to retain based on your desired level of dimensionality reduction. You can\n",
    "     decide this based on the explained variance ratio or by setting a fixed number of components.\n",
    "        \n",
    "5.Explained Variance Analysis:\n",
    "\n",
    "    ~Examine the explained variance ratio associated with each principal component. The explained variance ratio indicates\n",
    "     the proportion of the total variance in the data that is captured by each component.\n",
    "    ~Plot a cumulative explained variance curve to visualize how many principal components are needed to retain a significant\n",
    "     portion of the variance. You can typically choose a threshold (e.g., 95% variance explained) to determine the number of\n",
    "    components to keep.\n",
    "    \n",
    "6.Select Principal Components:\n",
    "\n",
    "    ~Based on the analysis of the explained variance, select the optimal number of principal components to retain. These\n",
    "     components will serve as the reduced feature set for your model.\n",
    "    ~Retaining fewer components will lead to dimensionality reduction but may also result in some loss of information. \n",
    "     However, the retained components will capture the most essential patterns in the data.\n",
    "        \n",
    "7.Transform the Data:\n",
    "\n",
    "    ~Transform the original standardized data using the selected principal components. This transformation results in a\n",
    "     reduced-dimensional dataset with the same number of rows but fewer features.\n",
    "        \n",
    "8.Model Building:\n",
    "\n",
    "    ~Build your stock price prediction model using the reduced-dimensional dataset containing the principal components as\n",
    "     features.\n",
    "    ~Common machine learning algorithms like regression, time series analysis, or neural networks can be used for stock price\n",
    "     prediction, depending on the specific nature of your task.\n",
    "        \n",
    "9.Model Evaluation and Tuning:\n",
    "\n",
    "    ~Evaluate the performance of your model using appropriate metrics, such as mean squared error (MSE) or root mean squared\n",
    "     error (RMSE), and fine-tune the model as needed.\n",
    "    ~Consider conducting cross-validation to ensure the model's robustness.\n",
    "    \n",
    "10.Interpretation and Reporting:\n",
    "\n",
    "    ~Interpret the results of your stock price prediction model, taking into account the contributions of the retained\n",
    "     principal components.\n",
    "    ~Communicate findings to stakeholders and use the model for real-world stock price prediction tasks.\n",
    "    \n",
    "PCA allows you to reduce the dimensionality of your dataset while preserving the most important patterns and relationships\n",
    "among the features, making it a valuable tool in stock price prediction and many other data analysis tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a14033-e6d9-4bc4-8bae-3f4f2c965b06",
   "metadata": {},
   "source": [
    "## Q7. For a dataset containing the following values: [1, 5, 10, 15, 20], perform Min-Max scaling to transform the values to a range of -1 to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c28a5b-9cb2-424d-b7b0-dd9782550d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "To perform Min-Max scaling on the given dataset and transform the values to a range of -1 to 1, you can follow these steps:\n",
    "\n",
    "1.Calculate the minimum (X min) and maximum (X max) values in the original dataset.\n",
    "2.Use the Min-Max scaling formula to scale each value in the dataset to the desired range.\n",
    "\n",
    "Let's apply these steps to the dataset [1, 5, 10, 15, 20]:\n",
    "\n",
    "1.Calculate X min and X max :\n",
    "        X min = 1 (minimum value in the dataset)\n",
    "        X max = 20 (maximum value in the dataset)\n",
    "        \n",
    "2.Apply Min-Max scaling to each value in the dataset using the formula:\n",
    "\n",
    "        Xnormalized = X−X min/X max − X min\n",
    "            ~For X=1:\n",
    "                Xnormalized = 1-1/20-1 = 0\n",
    "            ~For X=5:\n",
    "                Xnormalized = 5-1/20-1 = 4/19 ≈ 0.2105\n",
    "\n",
    "            ~For X=10:\n",
    "                Xnormalized = 10-1/20-1 = 9/19 ≈ 0.4737\n",
    "\n",
    "            ~For X=15:\n",
    "                Xnormalized = 15-1/20-1 = 14/19 ≈ 0.7368\n",
    "\n",
    "            ~For X=20:\n",
    "                Xnormalized = 20−1/20−1 =1\n",
    "\n",
    "Now, the Min-Max scaled values of the dataset [1, 5, 10, 15, 20] in the range of -1 to 1 are approximately [-1, -0.5789,\n",
    "-0.0526, 0.4737, 1]."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdff32c7-db91-4678-88d1-65c1b49780dd",
   "metadata": {},
   "source": [
    "## Q8. For a dataset containing the following features: [height, weight, age, gender, blood pressure], perform.Feature Extraction using PCA. How many principal components would you choose to retain, and why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f13114-e0ef-4282-bc55-d2d022103582",
   "metadata": {},
   "outputs": [],
   "source": [
    "Deciding how many principal components to retain in a PCA-based feature extraction depends on various factors, including your\n",
    "project's goals, the amount of variance you want to retain, and the trade-off between dimensionality reduction and information\n",
    "loss. Here are the general steps to help you determine how many principal components to retain for the given dataset with\n",
    "features [height, weight, age, gender, blood pressure]:\n",
    "\n",
    "1.Standardize the Data:\n",
    "\n",
    "    ~Start by standardizing the data (mean centering and scaling to unit variance) so that all features have equal importance \n",
    "     in the PCA.\n",
    "        \n",
    "2.PCA Transformation:\n",
    "\n",
    "    ~Apply PCA to the standardized data to obtain the eigenvalues and eigenvectors.\n",
    "    \n",
    "3.Analyze the Explained Variance:\n",
    "\n",
    "    ~Examine the explained variance associated with each principal component. The explained variance indicates how much of\n",
    "     the total variance in the data is captured by each component.\n",
    "    ~Calculate the cumulative explained variance by summing the explained variance values as you go through the components.\n",
    "    \n",
    "4.Decide on the Number of Components:\n",
    "\n",
    "    ~Determine how much variance you want to retain. This depends on your project's requirements and the trade-off between\n",
    "     dimensionality reduction and information retention.\n",
    "    ~A common rule of thumb is to retain a sufficient number of components to capture a significant portion of the total \n",
    "     variance, such as 95% or 99% of the variance.\n",
    "        \n",
    "5.Elbow Plot or Scree Plot (Optional):\n",
    "\n",
    "    ~You can create an elbow plot or scree plot, which shows the explained variance as a function of the number of\n",
    "     components. Look for an \"elbow point\" in the plot, where the explained variance starts to level off. This can help you\n",
    "    decide on an appropriate number of components.\n",
    "    \n",
    "6.Retain the Principal Components:\n",
    "\n",
    "    ~Based on your decision from step 4, retain the chosen number of principal components. These components will serve as the\n",
    "     reduced feature set for your analysis.\n",
    "        \n",
    "The number of principal components to retain ultimately depends on your specific project's goals and constraints. You may \n",
    "choose to retain fewer components to achieve higher dimensionality reduction or more components to retain a higher percentage\n",
    "of the original variance and information. It's important to strike a balance that suits your particular use case.\n",
    "\n",
    "For example, if you decide to retain 95% of the variance, you can examine the cumulative explained variance and choose the\n",
    "minimum number of components that achieve this level of retention. If, after analysis, you find that retaining three principal\n",
    "components captures 95% of the variance, you would choose to retain three principal components for feature extraction.\n",
    "However, the exact number may vary based on the actual data and analysis results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
